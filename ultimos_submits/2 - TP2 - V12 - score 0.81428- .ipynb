{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from os import path\n",
    "from PIL import Image\n",
    "\n",
    "import gensim\n",
    "\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout, Concatenate, LeakyReLU, GRU\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input, Model, regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"dataset/train.csv\")\n",
    "df_test= pd.read_csv(\"dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contador de palabras\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# palabras unicas\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# contador de stopwords\n",
    "df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "\n",
    "# promedio del largo de palabras\n",
    "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# contador de caracteres\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# contador de puntuaciones\n",
    "df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# calculo el ratio de las stopwords\n",
    "df_train['stopword_ratio'] = df_train['stop_word_count'] / df_train['word_count']\n",
    "df_test['stopword_ratio'] = df_test['stop_word_count'] / df_test['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimino columnas que no voy a utilizar\n",
    "test_id = df_test['id']\n",
    "\n",
    "columns = {'id','keyword','location'}\n",
    "df_train = df_train.drop(columns = columns)\n",
    "df_test = df_test.drop(columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>char_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>stopword_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>88</td>\n",
       "      <td>2</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>125</td>\n",
       "      <td>5</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>6.263158</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>6.307692</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  word_count  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1          13   \n",
       "1                Forest fire near La Ronge Sask. Canada       1           7   \n",
       "2     All residents asked to 'shelter in place' are ...       1          22   \n",
       "3     13,000 people receive #wildfires evacuation or...       1           8   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1          16   \n",
       "...                                                 ...     ...         ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1          11   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1          20   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1           8   \n",
       "7611  Police investigating after an e-bike collided ...       1          19   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1          13   \n",
       "\n",
       "      unique_word_count  stop_word_count  mean_word_length  char_count  \\\n",
       "0                    13                6          4.384615          69   \n",
       "1                     7                0          4.571429          38   \n",
       "2                    20               11          5.090909         133   \n",
       "3                     8                1          7.125000          65   \n",
       "4                    15                7          4.500000          88   \n",
       "...                 ...              ...               ...         ...   \n",
       "7608                 11                2          6.636364          83   \n",
       "7609                 17                9          5.300000         125   \n",
       "7610                  8                2          7.250000          65   \n",
       "7611                 19                5          6.263158         137   \n",
       "7612                 13                3          6.307692          94   \n",
       "\n",
       "      punctuation_count  stopword_ratio  \n",
       "0                     1        0.461538  \n",
       "1                     1        0.000000  \n",
       "2                     3        0.500000  \n",
       "3                     2        0.125000  \n",
       "4                     2        0.437500  \n",
       "...                 ...             ...  \n",
       "7608                  5        0.181818  \n",
       "7609                  5        0.450000  \n",
       "7610                 11        0.250000  \n",
       "7611                  5        0.263158  \n",
       "7612                  7        0.230769  \n",
       "\n",
       "[7613 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con el fin de agilizar el tiempo para la construccion de algunos filtros se han recolectado de diferentes fuentes\n",
    "\n",
    "def limpieza(text):\n",
    "            \n",
    "    # elimino caracteres especiales\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # Reemplazo contracciones\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"We're\", \"We are\", text)\n",
    "    text = re.sub(r\"That's\", \"That is\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"they're\", \"they are\", text)\n",
    "    text = re.sub(r\"Can't\", \"Cannot\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"What's\", \"What is\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "    text = re.sub(r\"There's\", \"There is\", text)\n",
    "    text = re.sub(r\"He's\", \"He is\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"You're\", \"You are\", text)\n",
    "    text = re.sub(r\"I'M\", \"I am\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\"Isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"Here's\", \"Here is\", text)\n",
    "    text = re.sub(r\"you've\", \"you have\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n",
    "    text = re.sub(r\"we're\", \"we are\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"we've\", \"we have\", text)\n",
    "    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n",
    "    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n",
    "    text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n",
    "    text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n",
    "    text = re.sub(r\"y'all\", \"you all\", text)\n",
    "    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n",
    "    text = re.sub(r\"would've\", \"would have\", text)\n",
    "    text = re.sub(r\"it'll\", \"it will\", text)\n",
    "    text = re.sub(r\"we'll\", \"we will\", text)\n",
    "    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n",
    "    text = re.sub(r\"We've\", \"We have\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"Y'all\", \"You all\", text)\n",
    "    text = re.sub(r\"Weren't\", \"Were not\", text)\n",
    "    text = re.sub(r\"Didn't\", \"Did not\", text)\n",
    "    text = re.sub(r\"they'll\", \"they will\", text)\n",
    "    text = re.sub(r\"they'd\", \"they would\", text)\n",
    "    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
    "    text = re.sub(r\"That\\x89Ûªs\", \"That is\", text)\n",
    "    text = re.sub(r\"they've\", \"they have\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"should've\", \"should have\", text)\n",
    "    text = re.sub(r\"You\\x89Ûªre\", \"You are\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"Don\\x89Ûªt\", \"Do not\", text)\n",
    "    text = re.sub(r\"we'd\", \"we would\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"weren't\", \"were not\", text)\n",
    "    text = re.sub(r\"They're\", \"They are\", text)\n",
    "    text = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªll\", \"you will\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªd\", \"I would\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"you're\", \"you are\", text)\n",
    "    text = re.sub(r\"i've\", \"I have\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"I've\", \"I have\", text)\n",
    "    text = re.sub(r\"Don't\", \"do not\", text)\n",
    "    text = re.sub(r\"I'll\", \"I will\", text)\n",
    "    text = re.sub(r\"I'd\", \"I would\", text)\n",
    "    text = re.sub(r\"Let's\", \"Let us\", text)\n",
    "    text = re.sub(r\"you'd\", \"You would\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"Ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "    text = re.sub(r\"Could've\", \"Could have\", text)\n",
    "    text = re.sub(r\"youve\", \"you have\", text)  \n",
    "    text = re.sub(r\"donå«t\", \"do not\", text)\n",
    "    \n",
    "    #segunda lista (acronimos)\n",
    "    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n",
    "    text = re.sub(r\"azwx\", \"Arizona Weather\", text)  \n",
    "    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n",
    "    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)      \n",
    "    text = re.sub(r\"gawx\", \"Georgia Weather\", text)  \n",
    "    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)  \n",
    "    text = re.sub(r\"cawx\", \"California Weather\", text)\n",
    "    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text) \n",
    "    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)\n",
    "    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n",
    "    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)  \n",
    "    text = re.sub(r\"lmao\", \"laughing my ass off\", text)  \n",
    "    text = re.sub(r\"amirite\", \"am I right\", text)\n",
    "    \n",
    "    #and some typos/abbreviations\n",
    "    text = re.sub(r\"w/e\", \"whatever\", text)\n",
    "    text = re.sub(r\"w/\", \"with\", text)\n",
    "    text = re.sub(r\"USAgov\", \"USA government\", text)\n",
    "    text = re.sub(r\"recentlu\", \"recently\", text)\n",
    "    text = re.sub(r\"Ph0tos\", \"Photos\", text)\n",
    "    text = re.sub(r\"exp0sed\", \"exposed\", text)\n",
    "    text = re.sub(r\"<3\", \"love\", text)\n",
    "    text = re.sub(r\"amageddon\", \"armageddon\", text)\n",
    "    text = re.sub(r\"Trfc\", \"Traffic\", text)\n",
    "    text = re.sub(r\"WindStorm\", \"Wind Storm\", text)\n",
    "    text = re.sub(r\"16yr\", \"16 year\", text)\n",
    "    text = re.sub(r\"TRAUMATISED\", \"traumatized\", text)\n",
    "    \n",
    "    #hashtags and usernames\n",
    "    text = re.sub(r\"IranDeal\", \"Iran Deal\", text)\n",
    "    text = re.sub(r\"ArianaGrande\", \"Ariana Grande\", text)\n",
    "    text = re.sub(r\"camilacabello97\", \"camila cabello\", text) \n",
    "    text = re.sub(r\"RondaRousey\", \"Ronda Rousey\", text)     \n",
    "    text = re.sub(r\"MTVHottest\", \"MTV Hottest\", text)\n",
    "    text = re.sub(r\"TrapMusic\", \"Trap Music\", text)\n",
    "    text = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", text)\n",
    "    text = re.sub(r\"PantherAttack\", \"Panther Attack\", text)\n",
    "    text = re.sub(r\"StrategicPatience\", \"Strategic Patience\", text)\n",
    "    text = re.sub(r\"socialnews\", \"social news\", text)\n",
    "    text = re.sub(r\"IDPs:\", \"Internally Displaced People :\", text)\n",
    "    text = re.sub(r\"ArtistsUnited\", \"Artists United\", text)\n",
    "    text = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", text)\n",
    "    text = re.sub(r\"jimmyfallon\", \"jimmy fallon\", text)\n",
    "    text = re.sub(r\"justinbieber\", \"justin bieber\", text)  \n",
    "    text = re.sub(r\"Time2015\", \"Time 2015\", text)\n",
    "    text = re.sub(r\"djicemoon\", \"dj icemoon\", text)\n",
    "    text = re.sub(r\"LivingSafely\", \"Living Safely\", text)\n",
    "    text = re.sub(r\"FIFA16\", \"Fifa 2016\", text)\n",
    "    text = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", text)\n",
    "    text = re.sub(r\"bbcnews\", \"bbc news\", text)\n",
    "    text = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", text)\n",
    "    text = re.sub(r\"c4news\", \"c4 news\", text)\n",
    "    text = re.sub(r\"MUDSLIDE\", \"mudslide\", text)\n",
    "    text = re.sub(r\"NoSurrender\", \"No Surrender\", text)\n",
    "    text = re.sub(r\"NotExplained\", \"Not Explained\", text)\n",
    "    text = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", text)\n",
    "    text = re.sub(r\"LondonFire\", \"London Fire\", text)\n",
    "    text = re.sub(r\"KOTAWeather\", \"KOTA Weather\", text)\n",
    "    text = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", text)\n",
    "    text = re.sub(r\"KOIN6News\", \"KOIN 6 News\", text)\n",
    "    text = re.sub(r\"LiveOnK2\", \"Live On K2\", text)\n",
    "    text = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", text)\n",
    "    text = re.sub(r\"nikeplus\", \"nike plus\", text)\n",
    "    text = re.sub(r\"david_cameron\", \"David Cameron\", text)\n",
    "    text = re.sub(r\"peterjukes\", \"Peter Jukes\", text)\n",
    "    text = re.sub(r\"MikeParrActor\", \"Michael Parr\", text)\n",
    "    text = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", text)\n",
    "    text = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", text)\n",
    "    text = re.sub(r\"realmandyrain\", \"Mandy Rain\", text)\n",
    "    text = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", text)\n",
    "    text = re.sub(r\"ApolloBrown\", \"Apollo Brown\", text)\n",
    "    text = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", text)\n",
    "    text = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", text)\n",
    "    text = re.sub(r\"AbbsWinston\", \"Abbs Winston\", text)\n",
    "    text = re.sub(r\"ShaunKing\", \"Shaun King\", text)\n",
    "    text = re.sub(r\"MeekMill\", \"Meek Mill\", text)\n",
    "    text = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", text)\n",
    "    text = re.sub(r\"GRupdates\", \"GR updates\", text)\n",
    "    text = re.sub(r\"SouthDowns\", \"South Downs\", text)\n",
    "    text = re.sub(r\"braininjury\", \"brain injury\", text)\n",
    "    text = re.sub(r\"auspol\", \"Australian politics\", text)\n",
    "    text = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", text)\n",
    "    text = re.sub(r\"calgaryweather\", \"Calgary Weather\", text)\n",
    "    text = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", text)\n",
    "    text = re.sub(r\"edsheeran\", \"Ed Sheeran\", text)\n",
    "    text = re.sub(r\"TrueHeroes\", \"True Heroes\", text)\n",
    "    text = re.sub(r\"ComplexMag\", \"Complex Magazine\", text)\n",
    "    text = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", text)\n",
    "    text = re.sub(r\"CityofCalgary\", \"City of Calgary\", text)\n",
    "    text = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", text)\n",
    "    text = re.sub(r\"SummerFate\", \"Summer Fate\", text)\n",
    "    text = re.sub(r\"RAmag\", \"Royal Academy Magazine\", text)\n",
    "    text = re.sub(r\"offers2go\", \"offers to go\", text)\n",
    "    text = re.sub(r\"ModiMinistry\", \"Modi Ministry\", text)\n",
    "    text = re.sub(r\"TAXIWAYS\", \"taxi ways\", text)\n",
    "    text = re.sub(r\"Calum5SOS\", \"Calum Hood\", text)\n",
    "    text = re.sub(r\"JamesMelville\", \"James Melville\", text)\n",
    "    text = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", text)\n",
    "    text = re.sub(r\"textLikeItsSeptember11th2001\", \"text like it is september 11th 2001\", text)\n",
    "    text = re.sub(r\"cbplawyers\", \"cbp lawyers\", text)\n",
    "    text = re.sub(r\"fewmoretexts\", \"few more texts\", text)\n",
    "    text = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", text)\n",
    "    text = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", text)\n",
    "    text = re.sub(r\"onlinecommunities\", \"online communities\", text)\n",
    "    text = re.sub(r\"humanconsumption\", \"human consumption\", text)\n",
    "    text = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", text)\n",
    "    text = re.sub(r\"Meat-Loving\", \"Meat Loving\", text)\n",
    "    text = re.sub(r\"facialabuse\", \"facial abuse\", text)\n",
    "    text = re.sub(r\"LakeCounty\", \"Lake County\", text)\n",
    "    text = re.sub(r\"BeingAuthor\", \"Being Author\", text)\n",
    "    text = re.sub(r\"withheavenly\", \"with heavenly\", text)\n",
    "    text = re.sub(r\"thankU\", \"thank you\", text)\n",
    "    text = re.sub(r\"iTunesMusic\", \"iTunes Music\", text)\n",
    "    text = re.sub(r\"OffensiveContent\", \"Offensive Content\", text)\n",
    "    text = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", text)\n",
    "    text = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", text)\n",
    "    text = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", text)\n",
    "    text = re.sub(r\"animalrescue\", \"animal rescue\", text)\n",
    "    text = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", text)\n",
    "    text = re.sub(r\"Throwingknifes\", \"Throwing knives\", text)\n",
    "    text = re.sub(r\"GodsLove\", \"God's Love\", text)\n",
    "    text = re.sub(r\"bookboost\", \"book boost\", text)\n",
    "    text = re.sub(r\"ibooklove\", \"I book love\", text)\n",
    "    text = re.sub(r\"NestleIndia\", \"Nestle India\", text)\n",
    "    text = re.sub(r\"realDonaldTrump\", \"Donald Trump\", text)\n",
    "    text = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", text)\n",
    "    text = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", text)\n",
    "    text = re.sub(r\"weathernetwork\", \"weather network\", text)\n",
    "    text = re.sub(r\"GOPDebate\", \"GOP Debate\", text)\n",
    "    text = re.sub(r\"RickPerry\", \"Rick Perry\", text)\n",
    "    text = re.sub(r\"frontpage\", \"front page\", text)\n",
    "    text = re.sub(r\"NewsIntexts\", \"News In texts\", text)\n",
    "    text = re.sub(r\"ViralSpell\", \"Viral Spell\", text)\n",
    "    text = re.sub(r\"til_now\", \"until now\", text)\n",
    "    text = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", text)\n",
    "    text = re.sub(r\"ZippedNews\", \"Zipped News\", text)\n",
    "    text = re.sub(r\"MicheleBachman\", \"Michele Bachman\", text)\n",
    "    text = re.sub(r\"53inch\", \"53 inch\", text)\n",
    "    text = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", text)\n",
    "    text = re.sub(r\"abstorm\", \"Alberta Storm\", text)\n",
    "    text = re.sub(r\"Beyhive\", \"Beyonce hive\", text)\n",
    "    text = re.sub(r\"RockyFire\", \"Rocky Fire\", text)\n",
    "    text = re.sub(r\"Listen/Buy\", \"Listen / Buy\", text)\n",
    "    text = re.sub(r\"ArtistsUnited\", \"Artists United\", text)\n",
    "    text = re.sub(r\"ENGvAUS\", \"England vs Australia\", text)\n",
    "    text = re.sub(r\"ScottWalker\", \"Scott Walker\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df_train['text']=df_train['text'].apply(lambda x: limpieza(x))\n",
    "df_test['text']=df_test['text'].apply(lambda x: limpieza(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminacion(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "     \n",
    "    ## clean urls \n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url.sub(r'',text)\n",
    "    \n",
    "    url = re.compile(r'http?://\\S+|www\\.\\S+')\n",
    "    text = url.sub(r'',text)\n",
    "    \n",
    "    ## remove html \n",
    "    html=re.compile(r'<.*?>') \n",
    "    html.sub(r'',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df_train['text']=df_train['text'].apply(lambda x: eliminacion(x))\n",
    "df_test['text']=df_test['text'].apply(lambda x: eliminacion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train,df_test])\n",
    "corpus = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28871 unique tokens.\n",
      "Shape of data tensor: (10876, 32)\n",
      "Shape of label tensor: (7613,)\n"
     ]
    }
   ],
   "source": [
    "#define tokenizer options\n",
    "tokenizer_obj = Tokenizer()     \n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences = tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences)\n",
    "labels = df_train['target']\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "nlp_train = data[:len(df_train)]\n",
    "labels = labels\n",
    "nlp_test = data[len(df_train):]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = data.shape[1]\n",
    "\n",
    "\n",
    "#Found 28871 unique tokens.\n",
    "#Shape of data tensor: (10876, 32)\n",
    "#Shape of label tensor: (7613,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors in the GloVe library\n"
     ]
    }
   ],
   "source": [
    "#convierto a secuencias\n",
    "embedding_dict={}\n",
    "#with open('dataset/glove.twitter.27B.100d.txt','r') as f:\n",
    "#with open('dataset/glove.42B.300d.txt','r') as f:\n",
    "with open('dataset/glove.twitter.27B.200d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors in the GloVe library' % len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,200))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matriz incrsutada es de dimension (28872, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"La matriz incrsutada es de dimension\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(num_words, 200, weights = [embedding_matrix],\n",
    "                     input_length = MAX_SEQUENCE_LENGTH, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#agrego los features calculados anteriormente\n",
    "meta_train = StandardScaler().fit_transform(df_train.iloc[:, 2:])\n",
    "meta_test = StandardScaler().fit_transform(df_test.iloc[:, 1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para crear el modelo lstm\n",
    "def crear_lstm(spatial_dropout, dropout, recurrent_dropout, learning_rate, bidirectional = False):\n",
    "    \n",
    "    #define activation\n",
    "    activation = LeakyReLU(alpha = 0.01)\n",
    "    \n",
    "    #define inputs\n",
    "    nlp_input = Input(shape = (MAX_SEQUENCE_LENGTH,), name = 'nlp_input')\n",
    "    meta_input_train = Input(shape = (7, ), name = 'meta_train')\n",
    "    emb = embedding(nlp_input)\n",
    "    emb = SpatialDropout1D(dropout)(emb)\n",
    "\n",
    "    #add LSTM layer  (lstm(100)((mejor)), lstm(150))\n",
    "    if bidirectional:\n",
    "        nlp_out = (Bidirectional(LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
    "                                 kernel_initializer = 'orthogonal')))(emb)\n",
    "    else:\n",
    "        nlp_out = (LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
    "                                 kernel_initializer = 'orthogonal'))(emb)        \n",
    "     \n",
    "    #add meta data    \n",
    "    x = Concatenate()([nlp_out, meta_input_train])\n",
    "    \n",
    "    #add output layer\n",
    "    x = Dropout(dropout)(x)\n",
    "    preds = Dense(1, activation='sigmoid', kernel_regularizer = regularizers.l2(1e-4))(x)\n",
    "    \n",
    "    #compile model\n",
    "    model = Model(inputs=[nlp_input , meta_input_train], outputs = preds)\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "nlp_input (InputLayer)          (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 200)      5774400     nlp_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 32, 200)      0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          240800      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "meta_train (InputLayer)         (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 207)          0           bidirectional_2[0][0]            \n",
      "                                                                 meta_train[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 207)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            208         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,015,408\n",
      "Trainable params: 241,008\n",
      "Non-trainable params: 5,774,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create our first model\n",
    "modelo = crear_lstm(spatial_dropout = .2, dropout = .2, recurrent_dropout = .2,\n",
    "                     learning_rate = 3e-4, bidirectional = True)\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/16\n",
      "6090/6090 [==============================] - 36s 6ms/step - loss: 0.5315 - accuracy: 0.7358 - val_loss: 0.4381 - val_accuracy: 0.8011\n",
      "Epoch 2/16\n",
      "6090/6090 [==============================] - 43s 7ms/step - loss: 0.4564 - accuracy: 0.7921 - val_loss: 0.4294 - val_accuracy: 0.8070\n",
      "Epoch 3/16\n",
      "6090/6090 [==============================] - 40s 7ms/step - loss: 0.4411 - accuracy: 0.7993 - val_loss: 0.4182 - val_accuracy: 0.8043\n",
      "Epoch 4/16\n",
      "6090/6090 [==============================] - 38s 6ms/step - loss: 0.4316 - accuracy: 0.8043 - val_loss: 0.4067 - val_accuracy: 0.8135\n",
      "Epoch 5/16\n",
      "6090/6090 [==============================] - 35s 6ms/step - loss: 0.4267 - accuracy: 0.8107 - val_loss: 0.4272 - val_accuracy: 0.8096\n",
      "Epoch 6/16\n",
      "6090/6090 [==============================] - 41s 7ms/step - loss: 0.4145 - accuracy: 0.8184 - val_loss: 0.4339 - val_accuracy: 0.8056\n",
      "Epoch 7/16\n",
      "6090/6090 [==============================] - 41s 7ms/step - loss: 0.4117 - accuracy: 0.8167 - val_loss: 0.4217 - val_accuracy: 0.8181\n",
      "Epoch 8/16\n",
      "6090/6090 [==============================] - 39s 6ms/step - loss: 0.4041 - accuracy: 0.8213 - val_loss: 0.4361 - val_accuracy: 0.8083\n",
      "Epoch 9/16\n",
      "6090/6090 [==============================] - 37s 6ms/step - loss: 0.4014 - accuracy: 0.8268 - val_loss: 0.4053 - val_accuracy: 0.8155\n",
      "Epoch 10/16\n",
      "6090/6090 [==============================] - 35s 6ms/step - loss: 0.3933 - accuracy: 0.8256 - val_loss: 0.4105 - val_accuracy: 0.8089\n",
      "Epoch 11/16\n",
      "6090/6090 [==============================] - 39s 6ms/step - loss: 0.3927 - accuracy: 0.8310 - val_loss: 0.4205 - val_accuracy: 0.8116\n",
      "Epoch 12/16\n",
      "6090/6090 [==============================] - 41s 7ms/step - loss: 0.3831 - accuracy: 0.8325 - val_loss: 0.4106 - val_accuracy: 0.8129\n",
      "Epoch 13/16\n",
      "6090/6090 [==============================] - 39s 6ms/step - loss: 0.3822 - accuracy: 0.8340 - val_loss: 0.4216 - val_accuracy: 0.8155\n",
      "Epoch 14/16\n",
      "6090/6090 [==============================] - 34s 6ms/step - loss: 0.3767 - accuracy: 0.8332 - val_loss: 0.4356 - val_accuracy: 0.8063\n",
      "Epoch 15/16\n",
      "6090/6090 [==============================] - 38s 6ms/step - loss: 0.3724 - accuracy: 0.8371 - val_loss: 0.4230 - val_accuracy: 0.8096\n",
      "Epoch 16/16\n",
      "6090/6090 [==============================] - 37s 6ms/step - loss: 0.3645 - accuracy: 0.8425 - val_loss: 0.4273 - val_accuracy: 0.8096\n"
     ]
    }
   ],
   "source": [
    "history = modelo.fit([nlp_train, meta_train], labels, validation_split = .2,\n",
    "                       epochs = 16, batch_size = 17, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/20\n",
      "6090/6090 [==============================] - 18s 3ms/step - loss: 0.6312 - accuracy: 0.6461 - val_loss: 0.5072 - val_accuracy: 0.7807\n",
      "Epoch 2/20\n",
      "6090/6090 [==============================] - 17s 3ms/step - loss: 0.4973 - accuracy: 0.7677 - val_loss: 0.4250 - val_accuracy: 0.8037\n",
      "Epoch 3/20\n",
      "6090/6090 [==============================] - 16s 3ms/step - loss: 0.4627 - accuracy: 0.7921 - val_loss: 0.4160 - val_accuracy: 0.8083\n",
      "Epoch 4/20\n",
      "6090/6090 [==============================] - 17s 3ms/step - loss: 0.4589 - accuracy: 0.7921 - val_loss: 0.4104 - val_accuracy: 0.8102\n",
      "Epoch 5/20\n",
      "6090/6090 [==============================] - 17s 3ms/step - loss: 0.4491 - accuracy: 0.7977 - val_loss: 0.4137 - val_accuracy: 0.8109\n",
      "Epoch 6/20\n",
      "6090/6090 [==============================] - 17s 3ms/step - loss: 0.4438 - accuracy: 0.8028 - val_loss: 0.4111 - val_accuracy: 0.8089\n",
      "Epoch 7/20\n",
      "6090/6090 [==============================] - 17s 3ms/step - loss: 0.4411 - accuracy: 0.8018 - val_loss: 0.4090 - val_accuracy: 0.8135\n",
      "Epoch 8/20\n",
      "6090/6090 [==============================] - 19s 3ms/step - loss: 0.4391 - accuracy: 0.8038 - val_loss: 0.4088 - val_accuracy: 0.8109\n",
      "Epoch 9/20\n",
      "6090/6090 [==============================] - 18s 3ms/step - loss: 0.4313 - accuracy: 0.8072 - val_loss: 0.4093 - val_accuracy: 0.8148\n",
      "Epoch 10/20\n",
      "6090/6090 [==============================] - 16s 3ms/step - loss: 0.4350 - accuracy: 0.8085 - val_loss: 0.4145 - val_accuracy: 0.8162\n",
      "Epoch 11/20\n",
      "6090/6090 [==============================] - 16s 3ms/step - loss: 0.4261 - accuracy: 0.8090 - val_loss: 0.4095 - val_accuracy: 0.8116\n",
      "Epoch 12/20\n",
      "6090/6090 [==============================] - 16s 3ms/step - loss: 0.4207 - accuracy: 0.8126 - val_loss: 0.4119 - val_accuracy: 0.8135\n",
      "Epoch 13/20\n",
      "6090/6090 [==============================] - 16s 3ms/step - loss: 0.4244 - accuracy: 0.8105 - val_loss: 0.4097 - val_accuracy: 0.8155\n",
      "Epoch 14/20\n",
      "6090/6090 [==============================] - 18s 3ms/step - loss: 0.4227 - accuracy: 0.8136 - val_loss: 0.4165 - val_accuracy: 0.8083\n",
      "Epoch 15/20\n",
      "6090/6090 [==============================] - 18s 3ms/step - loss: 0.4104 - accuracy: 0.8186 - val_loss: 0.4175 - val_accuracy: 0.8102\n",
      "Epoch 16/20\n",
      "6090/6090 [==============================] - 18s 3ms/step - loss: 0.4140 - accuracy: 0.8151 - val_loss: 0.4189 - val_accuracy: 0.8070\n",
      "Epoch 17/20\n",
      "6090/6090 [==============================] - 18s 3ms/step - loss: 0.4076 - accuracy: 0.8207 - val_loss: 0.4159 - val_accuracy: 0.8116\n",
      "Epoch 18/20\n",
      "6090/6090 [==============================] - 19s 3ms/step - loss: 0.4062 - accuracy: 0.8200 - val_loss: 0.4217 - val_accuracy: 0.8089\n",
      "Epoch 19/20\n",
      "6090/6090 [==============================] - 19s 3ms/step - loss: 0.4036 - accuracy: 0.8230 - val_loss: 0.4184 - val_accuracy: 0.8122\n",
      "Epoch 20/20\n",
      "6090/6090 [==============================] - 19s 3ms/step - loss: 0.4010 - accuracy: 0.8248 - val_loss: 0.4174 - val_accuracy: 0.8089\n"
     ]
    }
   ],
   "source": [
    "history2 = modelo.fit([nlp_train, meta_train], labels, validation_split = .2,\n",
    "                       epochs = 20, batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.838943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.959276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.890609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.930807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.507940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.080568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0.065838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0.091751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0.075537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      prob  target\n",
       "0   0  0.838943       1\n",
       "1   2  0.959276       1\n",
       "2   3  0.890609       1\n",
       "3   9  0.930807       1\n",
       "4  11  0.983741       1\n",
       "5  12  0.507940       1\n",
       "6  21  0.080568       0\n",
       "7  22  0.065838       0\n",
       "8  27  0.091751       0\n",
       "9  29  0.075537       0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create submission for complex lstm model\n",
    "submission_lstm = pd.DataFrame()\n",
    "submission_lstm['id'] = test_id\n",
    "submission_lstm['prob'] = modelo.predict([nlp_test, meta_test])\n",
    "submission_lstm['target'] = submission_lstm['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "submission_lstm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"dataset/sample_submission.csv\")\n",
    "submission['target'] = submission_lstm['target']\n",
    "submission.to_csv(\"dataset/submission12.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "model.add(LSTM(200, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=3e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           8157600   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 8,879,101\n",
      "Trainable params: 721,501\n",
      "Non-trainable params: 8,157,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tweet_pad[:df_train.shape[0]]\n",
    "test=tweet_pad[df_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (6090, 50)\n",
      "Shape of Validation  (1523, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,df_train['target'].values,test_size=0.2)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entro el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pablo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/25\n",
      " - 64s - loss: 0.5657 - accuracy: 0.7057 - val_loss: 0.5409 - val_accuracy: 0.7708\n",
      "Epoch 2/25\n",
      " - 63s - loss: 0.4594 - accuracy: 0.8033 - val_loss: 0.5044 - val_accuracy: 0.7873\n",
      "Epoch 3/25\n",
      " - 63s - loss: 0.4411 - accuracy: 0.8135 - val_loss: 0.4759 - val_accuracy: 0.7800\n",
      "Epoch 4/25\n",
      " - 63s - loss: 0.4306 - accuracy: 0.8190 - val_loss: 0.4778 - val_accuracy: 0.7807\n",
      "Epoch 5/25\n",
      " - 63s - loss: 0.4230 - accuracy: 0.8209 - val_loss: 0.4919 - val_accuracy: 0.7768\n",
      "Epoch 6/25\n",
      " - 63s - loss: 0.4192 - accuracy: 0.8258 - val_loss: 0.4954 - val_accuracy: 0.7820\n",
      "Epoch 7/25\n",
      " - 63s - loss: 0.4100 - accuracy: 0.8314 - val_loss: 0.4725 - val_accuracy: 0.7853\n",
      "Epoch 8/25\n",
      " - 68s - loss: 0.4074 - accuracy: 0.8325 - val_loss: 0.4766 - val_accuracy: 0.7807\n",
      "Epoch 9/25\n",
      " - 63s - loss: 0.4050 - accuracy: 0.8342 - val_loss: 0.4621 - val_accuracy: 0.7879\n",
      "Epoch 10/25\n",
      " - 63s - loss: 0.3997 - accuracy: 0.8309 - val_loss: 0.4881 - val_accuracy: 0.7794\n",
      "Epoch 11/25\n",
      " - 63s - loss: 0.3879 - accuracy: 0.8378 - val_loss: 0.4811 - val_accuracy: 0.7827\n",
      "Epoch 12/25\n",
      " - 63s - loss: 0.3939 - accuracy: 0.8322 - val_loss: 0.4672 - val_accuracy: 0.7853\n",
      "Epoch 13/25\n",
      " - 63s - loss: 0.3793 - accuracy: 0.8383 - val_loss: 0.4817 - val_accuracy: 0.7873\n",
      "Epoch 14/25\n",
      " - 63s - loss: 0.3703 - accuracy: 0.8460 - val_loss: 0.4611 - val_accuracy: 0.7932\n",
      "Epoch 15/25\n",
      " - 63s - loss: 0.3747 - accuracy: 0.8445 - val_loss: 0.4862 - val_accuracy: 0.7886\n",
      "Epoch 16/25\n",
      " - 63s - loss: 0.3648 - accuracy: 0.8488 - val_loss: 0.4616 - val_accuracy: 0.7820\n",
      "Epoch 17/25\n",
      " - 63s - loss: 0.3492 - accuracy: 0.8563 - val_loss: 0.5102 - val_accuracy: 0.7827\n",
      "Epoch 18/25\n",
      " - 63s - loss: 0.3455 - accuracy: 0.8548 - val_loss: 0.5607 - val_accuracy: 0.7833\n",
      "Epoch 19/25\n",
      " - 63s - loss: 0.3383 - accuracy: 0.8626 - val_loss: 0.5247 - val_accuracy: 0.7919\n",
      "Epoch 20/25\n",
      " - 63s - loss: 0.3319 - accuracy: 0.8634 - val_loss: 0.4992 - val_accuracy: 0.7827\n",
      "Epoch 21/25\n",
      " - 63s - loss: 0.3304 - accuracy: 0.8655 - val_loss: 0.4938 - val_accuracy: 0.7945\n",
      "Epoch 22/25\n",
      " - 63s - loss: 0.3099 - accuracy: 0.8752 - val_loss: 0.4816 - val_accuracy: 0.7945\n",
      "Epoch 23/25\n",
      " - 63s - loss: 0.3125 - accuracy: 0.8768 - val_loss: 0.5080 - val_accuracy: 0.7610\n",
      "Epoch 24/25\n",
      " - 63s - loss: 0.3136 - accuracy: 0.8737 - val_loss: 0.4942 - val_accuracy: 0.7925\n",
      "Epoch 25/25\n",
      " - 63s - loss: 0.3041 - accuracy: 0.8770 - val_loss: 0.4993 - val_accuracy: 0.7899\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=128,epochs=25,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparaciones \n",
    "\n",
    "#anterior loss: 0.3041 - accuracy: 0.8770 - val_loss: 0.4993 - val_accuracy: 0.7899  (0.80)\n",
    "\n",
    "#history loss: 0.4155 - accuracy: 0.8186 - val_loss: 0.4257 - val_accuracy: 0.8109 (batch 21, epochs 5)\n",
    "#history2 loss: 0.3268 - accuracy: 0.8614 - val_loss: 0.4602 - val_accuracy: 0.8056 (bacht 16, epochs 10)\n",
    "#history3  loss: 0.2563 - accuracy: 0.8954 - val_loss: 0.5022 - val_accuracy: 0.8043 (batch 128, epochs 20)\n",
    "#history4 loss: 0.3692 - accuracy: 0.8381 - val_loss: 0.4418 - val_accuracy: 0.8076\n",
    "#history5 loss: 0.3551 - accuracy: 0.8514 - val_loss: 0.4543 - val_accuracy: 0.8030\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_GloVe = model.predict(train)\n",
    "train_pred_GloVe_int = train_pred_GloVe.round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_GloVe = model.predict(test)\n",
    "test_pred_GloVe_int = test_pred_GloVe.round().astype('int')\n",
    "\n",
    "submission = pd.read_csv(\"dataset/sample_submission.csv\")\n",
    "submission['target'] = test_pred_GloVe_int\n",
    "submission.head(10)\n",
    "\n",
    "submission.to_csv(\"submission10.csv\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
