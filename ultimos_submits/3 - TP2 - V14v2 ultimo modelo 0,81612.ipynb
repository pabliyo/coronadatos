{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "import gensim\n",
    "\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout, Concatenate, LeakyReLU, GRU\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input, Model, regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"dataset/train.csv\")\n",
    "df_test= pd.read_csv(\"dataset/test.csv\")\n",
    "pd.set_option('display.max_colwidth', -1) #elimino limite de truncado para visualizacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agrego la palabra clave del tweet al texto\n",
    "df_train['keyword'].fillna('keyword', inplace = True)\n",
    "df_train['text'] = df_train['text'] + \" \" + df_train['keyword']\n",
    "\n",
    "df_test['keyword'].fillna('keyword', inplace = True)\n",
    "df_test['text'] = df_test['text'] + \" \" + df_test['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contador de palabras\n",
    "#df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "#df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# palabras unicas\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# contador de stopwords\n",
    "#df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "#df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "\n",
    "# promedio del largo de palabras\n",
    "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# contador de caracteres\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# contador de puntuaciones\n",
    "df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# calculo el ratio de las stopwords\n",
    "#df_train['stopword_ratio'] = df_train['stop_word_count'] / df_train['word_count']\n",
    "#df_test['stopword_ratio'] = df_test['stop_word_count'] / df_test['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con el fin de agilizar el tiempo para la construccion de algunos filtros se han recolectado de diferentes fuentes\n",
    "\n",
    "def limpieza(text):\n",
    "            \n",
    "    # elimino caracteres especiales\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # Reemplazo contracciones\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"We're\", \"We are\", text)\n",
    "    text = re.sub(r\"That's\", \"That is\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"they're\", \"they are\", text)\n",
    "    text = re.sub(r\"Can't\", \"Cannot\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"What's\", \"What is\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "    text = re.sub(r\"There's\", \"There is\", text)\n",
    "    text = re.sub(r\"He's\", \"He is\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"You're\", \"You are\", text)\n",
    "    text = re.sub(r\"I'M\", \"I am\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\"Isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"Here's\", \"Here is\", text)\n",
    "    text = re.sub(r\"you've\", \"you have\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n",
    "    text = re.sub(r\"we're\", \"we are\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"we've\", \"we have\", text)\n",
    "    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n",
    "    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n",
    "    text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n",
    "    text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n",
    "    text = re.sub(r\"y'all\", \"you all\", text)\n",
    "    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n",
    "    text = re.sub(r\"would've\", \"would have\", text)\n",
    "    text = re.sub(r\"it'll\", \"it will\", text)\n",
    "    text = re.sub(r\"we'll\", \"we will\", text)\n",
    "    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n",
    "    text = re.sub(r\"We've\", \"We have\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"Y'all\", \"You all\", text)\n",
    "    text = re.sub(r\"Weren't\", \"Were not\", text)\n",
    "    text = re.sub(r\"Didn't\", \"Did not\", text)\n",
    "    text = re.sub(r\"they'll\", \"they will\", text)\n",
    "    text = re.sub(r\"they'd\", \"they would\", text)\n",
    "    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
    "    text = re.sub(r\"That\\x89Ûªs\", \"That is\", text)\n",
    "    text = re.sub(r\"they've\", \"they have\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"should've\", \"should have\", text)\n",
    "    text = re.sub(r\"You\\x89Ûªre\", \"You are\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"Don\\x89Ûªt\", \"Do not\", text)\n",
    "    text = re.sub(r\"we'd\", \"we would\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"weren't\", \"were not\", text)\n",
    "    text = re.sub(r\"They're\", \"They are\", text)\n",
    "    text = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªll\", \"you will\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªd\", \"I would\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"you're\", \"you are\", text)\n",
    "    text = re.sub(r\"i've\", \"I have\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"I've\", \"I have\", text)\n",
    "    text = re.sub(r\"Don't\", \"do not\", text)\n",
    "    text = re.sub(r\"I'll\", \"I will\", text)\n",
    "    text = re.sub(r\"I'd\", \"I would\", text)\n",
    "    text = re.sub(r\"Let's\", \"Let us\", text)\n",
    "    text = re.sub(r\"you'd\", \"You would\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"Ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "    text = re.sub(r\"Could've\", \"Could have\", text)\n",
    "    text = re.sub(r\"youve\", \"you have\", text)  \n",
    "    text = re.sub(r\"donå«t\", \"do not\", text)\n",
    "    \n",
    "    #segunda lista (acronimos)\n",
    "    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n",
    "    text = re.sub(r\"azwx\", \"Arizona Weather\", text)  \n",
    "    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n",
    "    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)      \n",
    "    text = re.sub(r\"gawx\", \"Georgia Weather\", text)  \n",
    "    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)  \n",
    "    text = re.sub(r\"cawx\", \"California Weather\", text)\n",
    "    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text) \n",
    "    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)\n",
    "    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n",
    "    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)  \n",
    "    text = re.sub(r\"lmao\", \"laughing my ass off\", text)  \n",
    "    text = re.sub(r\"amirite\", \"am I right\", text)\n",
    "    \n",
    "    #and some typos/abbreviations\n",
    "    text = re.sub(r\"w/e\", \"whatever\", text)\n",
    "    text = re.sub(r\"w/\", \"with\", text)\n",
    "    text = re.sub(r\"USAgov\", \"USA government\", text)\n",
    "    text = re.sub(r\"recentlu\", \"recently\", text)\n",
    "    text = re.sub(r\"Ph0tos\", \"Photos\", text)\n",
    "    text = re.sub(r\"exp0sed\", \"exposed\", text)\n",
    "    text = re.sub(r\"<3\", \"love\", text)\n",
    "    text = re.sub(r\"amageddon\", \"armageddon\", text)\n",
    "    text = re.sub(r\"Trfc\", \"Traffic\", text)\n",
    "    text = re.sub(r\"WindStorm\", \"Wind Storm\", text)\n",
    "    text = re.sub(r\"16yr\", \"16 year\", text)\n",
    "    text = re.sub(r\"TRAUMATISED\", \"traumatized\", text)\n",
    "    \n",
    "    #hashtags and usernames\n",
    "    text = re.sub(r\"IranDeal\", \"Iran Deal\", text)\n",
    "    text = re.sub(r\"ArianaGrande\", \"Ariana Grande\", text)\n",
    "    text = re.sub(r\"camilacabello97\", \"camila cabello\", text) \n",
    "    text = re.sub(r\"RondaRousey\", \"Ronda Rousey\", text)     \n",
    "    text = re.sub(r\"MTVHottest\", \"MTV Hottest\", text)\n",
    "    text = re.sub(r\"TrapMusic\", \"Trap Music\", text)\n",
    "    text = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", text)\n",
    "    text = re.sub(r\"PantherAttack\", \"Panther Attack\", text)\n",
    "    text = re.sub(r\"StrategicPatience\", \"Strategic Patience\", text)\n",
    "    text = re.sub(r\"socialnews\", \"social news\", text)\n",
    "    text = re.sub(r\"IDPs:\", \"Internally Displaced People :\", text)\n",
    "    text = re.sub(r\"ArtistsUnited\", \"Artists United\", text)\n",
    "    text = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", text)\n",
    "    text = re.sub(r\"jimmyfallon\", \"jimmy fallon\", text)\n",
    "    text = re.sub(r\"justinbieber\", \"justin bieber\", text)  \n",
    "    text = re.sub(r\"Time2015\", \"Time 2015\", text)\n",
    "    text = re.sub(r\"djicemoon\", \"dj icemoon\", text)\n",
    "    text = re.sub(r\"LivingSafely\", \"Living Safely\", text)\n",
    "    text = re.sub(r\"FIFA16\", \"Fifa 2016\", text)\n",
    "    text = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", text)\n",
    "    text = re.sub(r\"bbcnews\", \"bbc news\", text)\n",
    "    text = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", text)\n",
    "    text = re.sub(r\"c4news\", \"c4 news\", text)\n",
    "    text = re.sub(r\"MUDSLIDE\", \"mudslide\", text)\n",
    "    text = re.sub(r\"NoSurrender\", \"No Surrender\", text)\n",
    "    text = re.sub(r\"NotExplained\", \"Not Explained\", text)\n",
    "    text = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", text)\n",
    "    text = re.sub(r\"LondonFire\", \"London Fire\", text)\n",
    "    text = re.sub(r\"KOTAWeather\", \"KOTA Weather\", text)\n",
    "    text = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", text)\n",
    "    text = re.sub(r\"KOIN6News\", \"KOIN 6 News\", text)\n",
    "    text = re.sub(r\"LiveOnK2\", \"Live On K2\", text)\n",
    "    text = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", text)\n",
    "    text = re.sub(r\"nikeplus\", \"nike plus\", text)\n",
    "    text = re.sub(r\"david_cameron\", \"David Cameron\", text)\n",
    "    text = re.sub(r\"peterjukes\", \"Peter Jukes\", text)\n",
    "    text = re.sub(r\"MikeParrActor\", \"Michael Parr\", text)\n",
    "    text = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", text)\n",
    "    text = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", text)\n",
    "    text = re.sub(r\"realmandyrain\", \"Mandy Rain\", text)\n",
    "    text = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", text)\n",
    "    text = re.sub(r\"ApolloBrown\", \"Apollo Brown\", text)\n",
    "    text = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", text)\n",
    "    text = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", text)\n",
    "    text = re.sub(r\"AbbsWinston\", \"Abbs Winston\", text)\n",
    "    text = re.sub(r\"ShaunKing\", \"Shaun King\", text)\n",
    "    text = re.sub(r\"MeekMill\", \"Meek Mill\", text)\n",
    "    text = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", text)\n",
    "    text = re.sub(r\"GRupdates\", \"GR updates\", text)\n",
    "    text = re.sub(r\"SouthDowns\", \"South Downs\", text)\n",
    "    text = re.sub(r\"braininjury\", \"brain injury\", text)\n",
    "    text = re.sub(r\"auspol\", \"Australian politics\", text)\n",
    "    text = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", text)\n",
    "    text = re.sub(r\"calgaryweather\", \"Calgary Weather\", text)\n",
    "    text = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", text)\n",
    "    text = re.sub(r\"edsheeran\", \"Ed Sheeran\", text)\n",
    "    text = re.sub(r\"TrueHeroes\", \"True Heroes\", text)\n",
    "    text = re.sub(r\"ComplexMag\", \"Complex Magazine\", text)\n",
    "    text = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", text)\n",
    "    text = re.sub(r\"CityofCalgary\", \"City of Calgary\", text)\n",
    "    text = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", text)\n",
    "    text = re.sub(r\"SummerFate\", \"Summer Fate\", text)\n",
    "    text = re.sub(r\"RAmag\", \"Royal Academy Magazine\", text)\n",
    "    text = re.sub(r\"offers2go\", \"offers to go\", text)\n",
    "    text = re.sub(r\"ModiMinistry\", \"Modi Ministry\", text)\n",
    "    text = re.sub(r\"TAXIWAYS\", \"taxi ways\", text)\n",
    "    text = re.sub(r\"Calum5SOS\", \"Calum Hood\", text)\n",
    "    text = re.sub(r\"JamesMelville\", \"James Melville\", text)\n",
    "    text = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", text)\n",
    "    text = re.sub(r\"textLikeItsSeptember11th2001\", \"text like it is september 11th 2001\", text)\n",
    "    text = re.sub(r\"cbplawyers\", \"cbp lawyers\", text)\n",
    "    text = re.sub(r\"fewmoretexts\", \"few more texts\", text)\n",
    "    text = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", text)\n",
    "    text = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", text)\n",
    "    text = re.sub(r\"onlinecommunities\", \"online communities\", text)\n",
    "    text = re.sub(r\"humanconsumption\", \"human consumption\", text)\n",
    "    text = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", text)\n",
    "    text = re.sub(r\"Meat-Loving\", \"Meat Loving\", text)\n",
    "    text = re.sub(r\"facialabuse\", \"facial abuse\", text)\n",
    "    text = re.sub(r\"LakeCounty\", \"Lake County\", text)\n",
    "    text = re.sub(r\"BeingAuthor\", \"Being Author\", text)\n",
    "    text = re.sub(r\"withheavenly\", \"with heavenly\", text)\n",
    "    text = re.sub(r\"thankU\", \"thank you\", text)\n",
    "    text = re.sub(r\"iTunesMusic\", \"iTunes Music\", text)\n",
    "    text = re.sub(r\"OffensiveContent\", \"Offensive Content\", text)\n",
    "    text = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", text)\n",
    "    text = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", text)\n",
    "    text = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", text)\n",
    "    text = re.sub(r\"animalrescue\", \"animal rescue\", text)\n",
    "    text = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", text)\n",
    "    text = re.sub(r\"Throwingknifes\", \"Throwing knives\", text)\n",
    "    text = re.sub(r\"GodsLove\", \"God's Love\", text)\n",
    "    text = re.sub(r\"bookboost\", \"book boost\", text)\n",
    "    text = re.sub(r\"ibooklove\", \"I book love\", text)\n",
    "    text = re.sub(r\"NestleIndia\", \"Nestle India\", text)\n",
    "    text = re.sub(r\"realDonaldTrump\", \"Donald Trump\", text)\n",
    "    text = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", text)\n",
    "    text = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", text)\n",
    "    text = re.sub(r\"weathernetwork\", \"weather network\", text)\n",
    "    text = re.sub(r\"GOPDebate\", \"GOP Debate\", text)\n",
    "    text = re.sub(r\"RickPerry\", \"Rick Perry\", text)\n",
    "    text = re.sub(r\"frontpage\", \"front page\", text)\n",
    "    text = re.sub(r\"NewsIntexts\", \"News In texts\", text)\n",
    "    text = re.sub(r\"ViralSpell\", \"Viral Spell\", text)\n",
    "    text = re.sub(r\"til_now\", \"until now\", text)\n",
    "    text = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", text)\n",
    "    text = re.sub(r\"ZippedNews\", \"Zipped News\", text)\n",
    "    text = re.sub(r\"MicheleBachman\", \"Michele Bachman\", text)\n",
    "    text = re.sub(r\"53inch\", \"53 inch\", text)\n",
    "    text = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", text)\n",
    "    text = re.sub(r\"abstorm\", \"Alberta Storm\", text)\n",
    "    text = re.sub(r\"Beyhive\", \"Beyonce hive\", text)\n",
    "    text = re.sub(r\"RockyFire\", \"Rocky Fire\", text)\n",
    "    text = re.sub(r\"Listen/Buy\", \"Listen / Buy\", text)\n",
    "    text = re.sub(r\"ArtistsUnited\", \"Artists United\", text)\n",
    "    text = re.sub(r\"ENGvAUS\", \"England vs Australia\", text)\n",
    "    text = re.sub(r\"ScottWalker\", \"Scott Walker\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df_train['text']=df_train['text'].apply(lambda x: limpieza(x))\n",
    "df_test['text']=df_test['text'].apply(lambda x: limpieza(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminacion(text):\n",
    "     \n",
    "    ## clean urls \n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url.sub(r'',text)\n",
    "    \n",
    "    ## elimino numeros\n",
    "    text = re.sub('\\d+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df_train['text']=df_train['text'].apply(lambda x: eliminacion(x))\n",
    "df_test['text']=df_test['text'].apply(lambda x: eliminacion(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_text_sequence(text):\n",
    "    \n",
    "    keras_text = text_to_word_sequence(text)\n",
    "    return keras_text\n",
    "\n",
    "df_train['keras_text'] = df_train['text'].apply(lambda x: keras_text_sequence(x))\n",
    "df_test['keras_text'] = df_test['text'].apply(lambda x: keras_text_sequence(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eliminar_stopwords(text):\n",
    "    sin_stopwords = [w for w in text if w not in stop_words]\n",
    "    return sin_stopwords\n",
    "\n",
    "df_train['sin_stopwords'] = df_train['keras_text'].apply(lambda x: eliminar_stopwords(x))\n",
    "df_test['sin_stopwords'] = df_test['keras_text'].apply(lambda x: eliminar_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def juntar(text):\n",
    "    juntado = ' '.join(text)\n",
    "    return juntado\n",
    "\n",
    "df_train['text_clean'] = df_train['sin_stopwords'].apply(lambda x: juntar(x))\n",
    "df_test['text_clean'] = df_test['sin_stopwords'].apply(lambda x: juntar(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_c48c327c_da43_11ea_a8a5_b316dead3193row0_col0{\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_c48c327c_da43_11ea_a8a5_b316dead3193row1_col0,#T_c48c327c_da43_11ea_a8a5_b316dead3193row2_col0{\n",
       "            background-color:  #e7e3f0;\n",
       "            color:  #000000;\n",
       "        }#T_c48c327c_da43_11ea_a8a5_b316dead3193row3_col0{\n",
       "            background-color:  #f1ebf5;\n",
       "            color:  #000000;\n",
       "        }#T_c48c327c_da43_11ea_a8a5_b316dead3193row4_col0{\n",
       "            background-color:  #fef6fb;\n",
       "            color:  #000000;\n",
       "        }#T_c48c327c_da43_11ea_a8a5_b316dead3193row5_col0{\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >target</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193level0_row0\" class=\"row_heading level0 row0\" >target</th>\n",
       "                        <td id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193level0_row1\" class=\"row_heading level0 row1\" >mean_word_length</th>\n",
       "                        <td id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193row1_col0\" class=\"data row1 col0\" >0.197556</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193level0_row2\" class=\"row_heading level0 row2\" >char_count</th>\n",
       "                        <td id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193row2_col0\" class=\"data row2 col0\" >0.195365</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193level0_row3\" class=\"row_heading level0 row3\" >punctuation_count</th>\n",
       "                        <td id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193row3_col0\" class=\"data row3 col0\" >0.143287</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193level0_row4\" class=\"row_heading level0 row4\" >id</th>\n",
       "                        <td id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193row4_col0\" class=\"data row4 col0\" >0.060781</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193level0_row5\" class=\"row_heading level0 row5\" >unique_word_count</th>\n",
       "                        <td id=\"T_c48c327c_da43_11ea_a8a5_b316dead3193row5_col0\" class=\"data row5 col0\" >0.056151</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0667f866d8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df_train.corr()\n",
    "corr[['target']].sort_values(by = 'target',ascending = False)\\\n",
    ".style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline  ## menor puntaje\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train,df_test])\n",
    "corpus = df['text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21166 unique tokens.\n",
      "Shape of data tensor: (10876, 26)\n",
      "Shape of label tensor: (7613,)\n"
     ]
    }
   ],
   "source": [
    "#define tokenizer options\n",
    "tokenizer_obj = Tokenizer()     \n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences = tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences)\n",
    "labels = df_train['target']\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "nlp_train = data[:len(df_train)]\n",
    "labels = labels\n",
    "nlp_test = data[len(df_train):]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = data.shape[1]\n",
    "\n",
    "\n",
    "#Found 28871 unique tokens. mejor predic\n",
    "#Shape of data tensor: (10876, 32)\n",
    "#Shape of label tensor: (7613,)\n",
    "\n",
    "#Found 21317 unique tokens.  con stop\n",
    "#Shape of data tensor: (10876, 35)\n",
    "#Shape of label tensor: (7613,)\n",
    "\n",
    "#Found 21166 unique tokens. sin stop\n",
    "#Shape of data tensor: (10876, 26)\n",
    "#Shape of label tensor: (7613,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors in the GloVe library\n"
     ]
    }
   ],
   "source": [
    "#convierto a secuencias\n",
    "embedding_dict={}\n",
    "#with open('dataset/glove.twitter.27B.100d.txt','r') as f:\n",
    "#with open('dataset/glove.42B.300d.txt','r') as f:   ## no lo pude leer, busque varias formas pero no me alcanzo la ram\n",
    "with open('dataset/glove.twitter.27B.200d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()\n",
    "del f\n",
    "gc.collect()\n",
    "\n",
    "print('Found %s word vectors in the GloVe library' % len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,200))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matriz incrsutada es de dimension (21167, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"La matriz incrsutada es de dimension\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(num_words, 200, weights = [embedding_matrix],\n",
    "                     input_length = MAX_SEQUENCE_LENGTH, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#agrego los features calculados anteriormente\n",
    "meta_train = StandardScaler().fit_transform(df_train.iloc[:, 5:9])\n",
    "meta_test = StandardScaler().fit_transform(df_test.iloc[:, 4:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para crear el modelo lstm\n",
    "def crear_lstm(spatial_dropout, dropout, recurrent_dropout, learning_rate, bidirectional = False):\n",
    "    \n",
    "    activation = LeakyReLU(alpha = 0.01)\n",
    "    \n",
    "    #define inputs\n",
    "    nlp_input = Input(shape = (MAX_SEQUENCE_LENGTH,), name = 'nlp_input')\n",
    "    meta_input_train = Input(shape = (4, ), name = 'meta_train')\n",
    "    emb = embedding(nlp_input)\n",
    "    emb = SpatialDropout1D(dropout)(emb)\n",
    "\n",
    "    #add LSTM layer  (lstm(100)((mejor)), lstm(150))\n",
    "    if bidirectional:\n",
    "        nlp_out = (Bidirectional(LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
    "                                 kernel_initializer = 'orthogonal')))(emb)\n",
    "    else:\n",
    "        nlp_out = (LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
    "                                 kernel_initializer = 'orthogonal'))(emb)        \n",
    "     \n",
    "    #add meta data    \n",
    "    x = Concatenate()([nlp_out, meta_input_train])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    preds = Dense(1, activation='sigmoid', kernel_regularizer = regularizers.l2(1e-4))(x)\n",
    "    \n",
    "    #compile model\n",
    "    model = Model(inputs=[nlp_input , meta_input_train], outputs = preds)\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "nlp_input (InputLayer)          (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 26, 200)      4233400     nlp_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 26, 200)      0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          240800      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "meta_train (InputLayer)         (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 204)          0           bidirectional_2[0][0]            \n",
      "                                                                 meta_train[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 204)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            205         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,474,405\n",
      "Trainable params: 241,005\n",
      "Non-trainable params: 4,233,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelo = crear_lstm(spatial_dropout = .2, dropout = .2, recurrent_dropout = .2,\n",
    "                     learning_rate = 3e-4, bidirectional = True)\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/10\n",
      "6090/6090 [==============================] - 103s 17ms/step - loss: 0.5140 - accuracy: 0.7525 - val_loss: 0.4142 - val_accuracy: 0.8142\n",
      "Epoch 2/10\n",
      "6090/6090 [==============================] - 95s 16ms/step - loss: 0.4539 - accuracy: 0.7952 - val_loss: 0.4174 - val_accuracy: 0.8116\n",
      "Epoch 3/10\n",
      "6090/6090 [==============================] - 97s 16ms/step - loss: 0.4442 - accuracy: 0.8062 - val_loss: 0.4323 - val_accuracy: 0.8024\n",
      "Epoch 4/10\n",
      "6090/6090 [==============================] - 96s 16ms/step - loss: 0.4295 - accuracy: 0.8138 - val_loss: 0.4161 - val_accuracy: 0.8194\n",
      "Epoch 5/10\n",
      "6090/6090 [==============================] - 118s 19ms/step - loss: 0.4226 - accuracy: 0.8151 - val_loss: 0.4152 - val_accuracy: 0.8148\n",
      "Epoch 6/10\n",
      "6090/6090 [==============================] - 108s 18ms/step - loss: 0.4077 - accuracy: 0.8235 - val_loss: 0.4156 - val_accuracy: 0.8162\n",
      "Epoch 7/10\n",
      "6090/6090 [==============================] - 97s 16ms/step - loss: 0.4106 - accuracy: 0.8227 - val_loss: 0.4190 - val_accuracy: 0.8135\n",
      "Epoch 8/10\n",
      "6090/6090 [==============================] - 106s 17ms/step - loss: 0.3944 - accuracy: 0.8305 - val_loss: 0.4343 - val_accuracy: 0.8122\n",
      "Epoch 9/10\n",
      "6090/6090 [==============================] - 103s 17ms/step - loss: 0.3991 - accuracy: 0.8278 - val_loss: 0.4483 - val_accuracy: 0.7938\n",
      "Epoch 10/10\n",
      "6090/6090 [==============================] - 101s 17ms/step - loss: 0.3927 - accuracy: 0.8296 - val_loss: 0.4099 - val_accuracy: 0.8201\n"
     ]
    }
   ],
   "source": [
    "history2 = modelo.fit([nlp_train, meta_train], labels, validation_split = .2,\n",
    "                       epochs = 20, batch_size = 128, verbose = 1) #0,81612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.785877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.810461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.948490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.950293</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.986241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.552918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.102386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0.115630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0.110467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0.089791</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      prob  target\n",
       "0  0   0.785877  1     \n",
       "1  2   0.810461  1     \n",
       "2  3   0.948490  1     \n",
       "3  9   0.950293  1     \n",
       "4  11  0.986241  1     \n",
       "5  12  0.552918  1     \n",
       "6  21  0.102386  0     \n",
       "7  22  0.115630  0     \n",
       "8  27  0.110467  0     \n",
       "9  29  0.089791  0     "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create submission for complex lstm model\n",
    "test_id = df_test['id']\n",
    "submission_lstm = pd.DataFrame()\n",
    "submission_lstm['id'] = test_id\n",
    "submission_lstm['prob'] = modelo.predict([nlp_test, meta_test])\n",
    "submission_lstm['target'] = submission_lstm['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "submission_lstm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"dataset/sample_submission.csv\")\n",
    "submission['target'] = submission_lstm['target']\n",
    "submission.to_csv(\"dataset/submission14v2.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
